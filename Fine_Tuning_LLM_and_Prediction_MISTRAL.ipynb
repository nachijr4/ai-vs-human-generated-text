{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0A4A68kO79zW"
   },
   "source": [
    "## Fine-Tuning LLM and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "7p-_S9Y98Gll"
   },
   "outputs": [],
   "source": [
    "efficient_finetuning_folder = \"./LLaMA-Efficient-Tuning\" #absolute path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "l1RmKZks8Moc"
   },
   "outputs": [],
   "source": [
    "def generate_ftune_job(account, cpus_per_task, mem, run_time, gpu, hf_token, efficient_finetuning_folder, stage, model_name_or_path, dataset, template, finetuning_type, lora_target, output_dir, per_device_train_batch_size, gradient_accumulation_steps, lr_scheduler_type, logging_steps, save_steps, learning_rate, num_train_epochs, plot_loss, fp16, filename):\n",
    "    text = f'''#!/bin/bash\n",
    "#SBATCH --account={account}\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --cpus-per-task={cpus_per_task}\n",
    "#SBATCH --mem={mem}\n",
    "#SBATCH --time={run_time}\n",
    "#SBATCH --partition=gpu\n",
    "#SBATCH --gres=gpu:{gpu}\n",
    "\n",
    "module purge\n",
    "module load gcc/11.3.0\n",
    "module load python/3.9.12\n",
    "module load nvidia-hpc-sdk\n",
    "module load git/2.36.1\n",
    "module load cuda/11.8.0\n",
    "\n",
    "export HF_TOKEN={hf_token}\n",
    "huggingface-cli login --token $HF_TOKEN\n",
    "\n",
    "cd {efficient_finetuning_folder}\n",
    "pip install --upgrade pip\n",
    "pip install -r requirements.txt\n",
    "\n",
    "CUDA_VISIBLE_DEVICES=0 python src/train_bash.py \\\\\n",
    "    --stage {stage} \\\\\n",
    "    --model_name_or_path  {model_name_or_path} \\\\\n",
    "    --do_train \\\\\n",
    "    --dataset {dataset} \\\\\n",
    "    --template {template} \\\\\n",
    "    --finetuning_type {finetuning_type} \\\\\n",
    "    --lora_target {lora_target} \\\n",
    "    --output_dir {output_dir} \\\\\n",
    "    --overwrite_cache \\\\\n",
    "    --per_device_train_batch_size {per_device_train_batch_size} \\\\\n",
    "    --gradient_accumulation_steps {gradient_accumulation_steps} \\\\\n",
    "    --lr_scheduler_type {lr_scheduler_type} \\\\\n",
    "    --logging_steps {logging_steps} \\\\\n",
    "    --save_steps {save_steps} \\\\\n",
    "    --learning_rate {learning_rate} \\\\\n",
    "    --num_train_epochs {num_train_epochs} \\\\\n",
    "    {'--plot_loss' if plot_loss else ''}\\\\\n",
    "    {'--fp16' if fp16 else ''}'''\n",
    "\n",
    "    with open(filename, 'w') as f:\n",
    "            f.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kwUYd8pr8QMR"
   },
   "source": [
    "- Update parameters of the generate_ftune_job function (if you are unsure about the values to use, leave them as they are or consult the Efficient Fine-Tuning repository or relevant literature for more information\n",
    "- After updating the parameters, run the following code to generate the job file to fine-tune the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L30WxCga8TmR"
   },
   "source": [
    "- Run the following code to start the job for fine-tuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "L4j2VVfV8aRs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 23089498\n"
     ]
    }
   ],
   "source": [
    "!sbatch ftune_long_llama.job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_ftune_job(\n",
    "    account=\"ll_774_951\", #the account you are charging resources to\n",
    "    cpus_per_task=8, #default\n",
    "    mem=\"64GB\",\n",
    "    run_time=\"10:00:00\", #update accordingly to how much time your predictions take; it varies depending on the question length and resources chosen\n",
    "    gpu=\"a100\", #a40 or a100, a100 is faster but usually busy\n",
    "    hf_token=\"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\", #create a Hugging Face account and substitute with your token\n",
    "    efficient_finetuning_folder=\"./LLaMA-Efficient-Tuning\", #the cloned repo folder\n",
    "    stage=\"sft\", #default\n",
    "    model_name_or_path=\"mistralai/Mistral-7B-Instruct-v0.1\", #you can change the model using the Hugging Face models\n",
    "    dataset=\"train_ai_detect\", #update with the name of the training dataset you uploaded\n",
    "    template=\"default\", #default\n",
    "    finetuning_type=\"lora\", #default\n",
    "    lora_target=\"q_proj,v_proj\", #default\n",
    "    output_dir=\"./ai_detect_mixtral\", #select an output directory for your trained model, choose a different folder for every train dataset, be sure to select an empty folder\n",
    "    per_device_train_batch_size=10, #default\n",
    "    gradient_accumulation_steps=4, #default\n",
    "    lr_scheduler_type=\"cosine\", #default\n",
    "    logging_steps=10,\n",
    "    save_steps=1,\n",
    "    learning_rate=\"5e-5\",\n",
    "    num_train_epochs=3.0,\n",
    "    plot_loss=True,\n",
    "    fp16=True,\n",
    "    filename=\"ftune_long_mixtral.job\", #the name of the job file to start the fine-tuning\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 23089499\n"
     ]
    }
   ],
   "source": [
    "!sbatch ftune_long_mixtral.job"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Guidance",
   "language": "python",
   "name": "ashwin"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
